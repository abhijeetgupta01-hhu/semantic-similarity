{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING functions for files\n",
    "\n",
    "# Read a normal text file\n",
    "def read_text(filename=None):\n",
    "    data = []\n",
    "    with open(filename, \"r\") as fp:\n",
    "        for eachLine in fp:\n",
    "            data.append(eachLine.strip())\n",
    "    return data\n",
    "        \n",
    "# Could not read the files using pandas dataframes directly (below). \n",
    "# Therefore, reading the data as normal text and then putting it into Dataframes later on.  \n",
    "def read_file(filename=None, header=None, cols=[], delimiter=None):\n",
    "    sep = \" \" if delimiter is None else delimiter\n",
    "        \n",
    "    if header is None:\n",
    "        if cols is not []:\n",
    "            assert isinstance(cols, list)\n",
    "            #cols = map(str, cols)\n",
    "        \n",
    "        dataframe = pd.read_csv(filename, header=None, names=cols, sep=sep)\n",
    "    else:\n",
    "        dataframe = pd.read_csv(filename, sep=sep)\n",
    "    return dataframe\n",
    "\n",
    "# WRITING functions\n",
    "def write_text(filename=None, data=None):\n",
    "    with open(filename, \"w\") as fp:\n",
    "        for eachLine in data:\n",
    "            fp.write(eachLine.strip() + \"\\n\")\n",
    "            \n",
    "# Make Dirs (even in a hierarchy)\n",
    "def make_dirs(path=None):\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok=False)\n",
    "        print(\"Dataset dir created\")\n",
    "    except FileExistsError as e:\n",
    "        print(\"Dataset directory already exists!\")\n",
    "    return\n",
    "\n",
    "\n",
    "# In[95]:\n",
    "\n",
    "\n",
    "# READ INPUT: CONSTITUENT DATA FILES\n",
    "def get_constituent_data(constituent_list, constituent_vectors):\n",
    "    # Read constituent list\n",
    "    const_list = pd.DataFrame({'constituents':read_text(filename=constituent_list)})\n",
    "    #print(const_list.info)\n",
    "\n",
    "    # Read constituent vectors\n",
    "    vectors = read_text(filename=constituent_vectors)\n",
    "    c_list = []\n",
    "    c_vectors = []\n",
    "    for eachLine in vectors:\n",
    "        parts = eachLine.split(\" \")\n",
    "        # First column is the constituent list \n",
    "        c_list.append(parts[0])\n",
    "        # Second column onwards is the entire vector for the constituent\n",
    "        c_vectors.append(parts[1:])\n",
    "    const_vectors = pd.DataFrame({'constituents':c_list, 'vectors':c_vectors}) \n",
    "    print(\"Vector matrix shape: \", const_vectors['vectors'].shape, len(const_vectors['vectors'][0]) )\n",
    "\n",
    "    # # SANITY CHECK\n",
    "    # # 1. No. of constituents are the same and entries are on the same row\n",
    "    assert (len(const_list[\"constituents\"]) == len(const_vectors[\"constituents\"]))\n",
    "    print(\"Read constituents (%d) and vectors (%d):\" % (len(const_list[\"constituents\"]),len(const_vectors[\"constituents\"])))\n",
    "\n",
    "    row_check = [True if const_list[\"constituents\"][i] == const_vectors[\"constituents\"][i] else False for i in range(len(const_list[\"constituents\"]))]\n",
    "    for i in range(len(const_list['constituents'])):\n",
    "        if not const_list[\"constituents\"][i] == const_vectors[\"constituents\"][i]:\n",
    "            print(const_list[\"constituents\"][i], const_vectors[\"constituents\"][i])\n",
    "\n",
    "    assert(all(row_check)), \"Constituent List and Vector files do not match\"\n",
    "    print(\"Constituent labels and order in both files match\")\n",
    "    \n",
    "    return const_list, const_vectors \n",
    "\n",
    "\n",
    "# In[96]:\n",
    "\n",
    "\n",
    "# READ OUTPUT: COMPOUND DATA FILES\n",
    "def get_compound_data(compound_list, compound_vectors):\n",
    "    \n",
    "    # Read compound list\n",
    "    comp_list = pd.DataFrame({'compound':read_text(filename=compound_list)})\n",
    "    #print(comp_list.info)\n",
    "\n",
    "    # Read constituent vectors\n",
    "    vectors = read_text(filename=compound_vectors)\n",
    "    c_list = []\n",
    "    c_vectors = []\n",
    "    for eachLine in vectors:\n",
    "        parts = eachLine.split(\" \")\n",
    "        # First column is the constituent list \n",
    "        c_list.append(parts[0])\n",
    "        # Second column onwards is the entire vector \n",
    "        c_vectors.append(parts[1:])\n",
    "    comp_vectors = pd.DataFrame({'compound':c_list, 'vectors':c_vectors}) \n",
    "    #print(comp_vectors.info)\n",
    "\n",
    "    # # SANITY CHECK\n",
    "    # # 1. No. of constituents are the same and entries are on the same row\n",
    "    assert (len(comp_list[\"compound\"]) == len(comp_vectors[\"compound\"]))\n",
    "    print(\"Read compounds (%d) and vectors (%d):\" % (len(comp_list[\"compound\"]),len(comp_vectors[\"compound\"])))\n",
    "\n",
    "    row_check = [True if comp_list[\"compound\"][i] == comp_vectors[\"compound\"][i] else False for i in range(len(comp_list[\"compound\"]))]\n",
    "    for i in range(len(comp_list['compound'])):\n",
    "        if not comp_list[\"compound\"][i] == comp_vectors[\"compound\"][i]:\n",
    "            print(comp_list[\"compound\"][i], comp_vectors[\"compound\"][i])\n",
    "\n",
    "    assert(all(row_check)), \"Compound List and Vector files do not match\"\n",
    "    print(\"Compound labels and order in both files match\")\n",
    "    \n",
    "    return comp_list, comp_vectors\n",
    "\n",
    "\n",
    "# In[97]:\n",
    "\n",
    "\n",
    "# # NORMALIZING THE CONSTITUENT AND COMPOUND VECTORS \n",
    "\n",
    "def normalize_data(normalization, ct_vec=None, cp_vec=None):\n",
    "    \n",
    "    if normalization:\n",
    "        if ct_vec is not None:\n",
    "            ct_vec['vectors'] = normalize(ct_vec['vectors'].values.tolist(), norm=\"l2\", axis=1).tolist()\n",
    "            print(\"SELECTED :: Constituent Normalization = True\")\n",
    "            \n",
    "        if cp_vec is not None:\n",
    "            #print(comp_vectors['vectors'].head(1))\n",
    "            cp_vec['vectors'] = normalize(cp_vec['vectors'].values.tolist(), norm=\"l2\", axis=1).tolist()\n",
    "            #print(comp_vectors['vectors'].head(1))\n",
    "            print(\"SELECTED :: Compound Normalization = True\")  \n",
    "    else:\n",
    "        \n",
    "        print(\"SELECTED :: Normalization = False\")\n",
    "    \n",
    "    return ct_vec, cp_vec\n",
    "\n",
    "\n",
    "# In[98]:\n",
    "\n",
    "\n",
    "def construct_vector(w1_vec, w2_vec, op=None):\n",
    "    # Perform the required operation on the vectors\n",
    "    if op == \"concatenation\":\n",
    "        return np.concatenate((w1_vec, w2_vec), axis=None)\n",
    "    if op == \"addition\":\n",
    "        return np.add(w1_vec, w2_vec)\n",
    "    if op == \"multiplication\":\n",
    "        return np.multiply(w1_vec, w2_vec)\n",
    "    \n",
    "# The function below works to create both the TRAINING and TEST datasets. \n",
    "# The only difference between the two is that TEST dataset 'does not' have any OUTPUT values, so we skip those\n",
    "\n",
    "def get_dataset(dataset_type, op=None, dataset=None, input_df=None, output_df=None):\n",
    "    # Create a dataset from the CAOSS training dataset\n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    skipped_data = []\n",
    "    datapoints = []\n",
    "    skipped = False\n",
    "    output_vec = []\n",
    "    \n",
    "    df_w1 = pd.DataFrame({'vec':[]})\n",
    "    df_w2 = pd.DataFrame({'vec':[]})\n",
    "    df_out = pd.DataFrame({'vec':[]})\n",
    "    data_in_out = pd.DataFrame({'input':[], 'output':[]}) \n",
    "    \n",
    "    for eachLine in dataset:\n",
    "        skipped = False\n",
    "        # Get the dataset elements, who's vectors have to be fetched from the dataframe \n",
    "        w1, w2, compound = eachLine.split(\" \")\n",
    "        # Get the vectors for input and output\n",
    "        # NOTE: Some compound vectors were not present in the 'compound' files, \n",
    "                # so they had to be looked up into the constituent files. \n",
    "        # If there was an exception in fetching the vectors, the datapoint was skipped\n",
    "        try:\n",
    "            w1_vec = input_df['vectors'][input_df.index[input_df['constituents'] == w1]].tolist()\n",
    "            w2_vec = input_df['vectors'][input_df.index[input_df['constituents'] == w2]].tolist()\n",
    "            \n",
    "            if not dataset_type == \"test\":\n",
    "                output_vec = output_df['vectors'][output_df.index[output_df['compound'] == compound]].tolist()\n",
    "                if len(output_vec) < 1:\n",
    "                    output_vec = input_df['vectors'][input_df.index[input_df['constituents'] == compound]].tolist()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error in fetching vectors for datapoint: %s\" % (eachLine))\n",
    "            skipped = True\n",
    "            \n",
    "        # If ANY input or output vector had LEN < 1 (i.e., could not be fetched), the datapoint was skipped!\n",
    "        if (len(w1_vec) < 1 or len(w2_vec) < 1) or (len(output_vec) < 1 and not dataset_type == \"test\"):\n",
    "            print(\"Skipping datapoint :: %s\" % (eachLine))\n",
    "            skipped = True\n",
    "        \n",
    "        if not skipped:\n",
    "            w1_vec = np.asarray(w1_vec[0], dtype=np.float)\n",
    "            w2_vec = np.asarray(w2_vec[0], dtype=np.float)\n",
    "#             print(len(w1_vec), len(w2_vec))\n",
    "            op_vec = construct_vector(w1_vec, w2_vec, op=op)\n",
    "#             print(\"Input op vec:\", len(op_vec))\n",
    "            input_data.append(op_vec)\n",
    "            datapoints.append(eachLine)\n",
    "            if not dataset_type == \"test\":\n",
    "                output_vec = np.asarray(output_vec[0], dtype=np.float)\n",
    "#                 print(\"Output op vec:\", len(output_vec))\n",
    "                output_data.append(output_vec)\n",
    "        else:\n",
    "            skipped_data.append(eachLine)\n",
    "            \n",
    "    return input_data, output_data, skipped_data, datapoints\n",
    "\n",
    "\n",
    "# In[99]:\n",
    "\n",
    "\n",
    "# READING CAOSS TRAINING DATA \n",
    "\n",
    "def create_training_datasets(dataset_path, training_data_file, operation, process, normalization, const_vectors=None, comp_vectors=None):\n",
    "    \n",
    "    training_dataset = read_text(filename=training_data_file)\n",
    "\n",
    "    # CREATING OUR Training DATASET\n",
    "    for eachOperation in operation:\n",
    "        print(\"TRAIN OPERATION :: \", eachOperation)\n",
    "        # Get the input, output vectors AND the skipped and ACTUAL training datapoints\n",
    "        input_data, output_data, skipped_data, datapoints = get_dataset(\"train\", op=eachOperation, \n",
    "                                                                        dataset=training_dataset, \n",
    "                                                                        input_df=const_vectors, \n",
    "                                                                        output_df=comp_vectors)\n",
    "\n",
    "        assert (len(input_data) == len(output_data)), \"INPUT and OUTPUT data should be of the same length\"\n",
    "        print(\"\\tTraining data extracted\")\n",
    "        print(\"\\t\", len(input_data), len(datapoints))\n",
    "        print(\"\\tInput vector dims:\", len(input_data[0]))\n",
    "        print(\"\\tOutput vector dims:\", len(output_data[0]))\n",
    "    #     print(input_data[0])\n",
    "    #     break\n",
    "\n",
    "        try:\n",
    "            final_training_data = [\"\\t\".join([datapoints[i], \",\".join(input_data[i]), \",\".join(output_data[i])]) for i in range(len(input_data))]\n",
    "        except TypeError as e:\n",
    "            # TypeError: sequence item 0: expected str instance, float found\n",
    "            # So, we explicitly TypeCast both input and output arrays as strings first!\n",
    "            final_training_data = [\"\\t\".join([datapoints[i], \",\".join(map(str, input_data[i])), \",\".join(map(str,output_data[i]))]) for i in range(len(input_data))]\n",
    "\n",
    "\n",
    "        # final_traning_data CAN be also be shuffled and divided into traning + validation data at this point\n",
    "\n",
    "        training_filename = \"_\".join([process, \"TRAIN\", operation[eachOperation]])\n",
    "        if normalization:\n",
    "            training_filename += \"_L2Norm\"\n",
    "        else:\n",
    "            training_filename += \"_UnNorm\" \n",
    "        training_filename += \".txt\"\n",
    "\n",
    "\n",
    "        write_text(filename=os.path.join(dataset_path, training_filename), data=final_training_data)\n",
    "        if len(skipped_data) > 0:\n",
    "            write_text(filename=os.path.join(dataset_path, \"SKIPPED_\"+process+\"_caoss_TRAIN_data.txt\"), data=skipped_data)\n",
    "        print(\"Training data saved! : %s\" % os.path.join(dataset_path, training_filename))\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "\n",
    "# READING the CAOSS TEST DATASET\n",
    "def create_test_datasets(dataset_path, test_data_file, operation, process, normalization, const_vectors=None, comp_vectors=None):\n",
    "    test_dataset = read_text(filename=test_data_file)\n",
    "\n",
    "    # CREATING OUR TEST DATASET\n",
    "    for eachOperation in operation:\n",
    "        print(\"TEST OPERATION :: \", eachOperation)\n",
    "        # Get the input, output vectors AND the skipped and ACTUAL test datapoints\n",
    "        input_data, output_data, skipped_data, datapoints = get_dataset(\"test\", op=eachOperation, \n",
    "                                                                        dataset=test_dataset, \n",
    "                                                                        input_df=const_vectors, \n",
    "                                                                        output_df=comp_vectors)\n",
    "\n",
    "    #     assert (len(input_data) == len(output_data)), \"INPUT (%d) and OUTPUT (%d) data should be of the same length\" % (len(input_data), len(output_data))\n",
    "        print(\"\\tTest data extracted\")\n",
    "        print(\"\\t\", len(input_data), len(datapoints))\n",
    "        print(\"\\tInput vector dims:\", len(input_data[0]))\n",
    "\n",
    "        try:\n",
    "            final_test_data = [\"\\t\".join([datapoints[i], \",\".join(input_data[i])]) for i in range(len(input_data))]\n",
    "        except TypeError as e:\n",
    "            # TypeError: sequence item 0: expected str instance, float found\n",
    "            # So, we explicitly TypeCast both input and output arrays as strings first!\n",
    "            final_test_data = [\"\\t\".join([datapoints[i], \",\".join(map(str, input_data[i]))]) for i in range(len(input_data))]\n",
    "\n",
    "\n",
    "        # final_traning_data CAN be also be shuffled and divided into traning + validation data at this point\n",
    "\n",
    "        test_filename = \"_\".join([process, \"TEST\", operation[eachOperation]])\n",
    "        if normalization:\n",
    "            test_filename += \"_L2Norm\"\n",
    "        else:\n",
    "            test_filename += \"_UnNorm\" \n",
    "        test_filename += \".txt\"\n",
    "\n",
    "        write_text(filename=os.path.join(dataset_path, test_filename), data=final_test_data)\n",
    "        if len(skipped_data) > 0:\n",
    "            write_text(filename=os.path.join(dataset_path, \"SKIPPED_\"+process+\"_caoss_TEST_data.txt\"), data=skipped_data)\n",
    "        print(\"Test data saved! : %s\" % os.path.join(dataset_path, test_filename))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# In[105]:\n",
    "\n",
    "def concatenate_multimodal_data(t_vectors, v_vectors, text, save_key):\n",
    "    \n",
    "    # print(text)\n",
    "    intersect = np.intersect1d(t_vectors[text], v_vectors[text]).tolist()\n",
    "    vectors = []\n",
    "    t_vecs = []\n",
    "    v_vecs = []\n",
    "    for i in range(len(intersect)):\n",
    "        term = intersect[i]\n",
    "        t_vec = t_vectors['vectors'][t_vectors.index[t_vectors[text] == term]].tolist()\n",
    "        v_vec = v_vectors['vectors'][v_vectors.index[v_vectors[text] == term]].tolist()\n",
    "        vectors.append(t_vec[0] + v_vec[0])\n",
    "        t_vecs.append(t_vec[0])\n",
    "        v_vecs.append(v_vec[0])\n",
    "        # print(len(t_vec[0]),len(v_vec[0]), len(vectors[i]))\n",
    "    assert len(intersect) == len(t_vecs) == len(v_vecs)\n",
    "    \n",
    "    c_vectors = pd.DataFrame({save_key:intersect, 'text_vecs':t_vecs, 'vision_vecs':v_vecs})\n",
    "    \n",
    "    return c_vectors, intersect, t_vecs, v_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS ::  textvision\n",
      "Processing text vision data\n",
      "Vector matrix shape:  (2387,) 400\n",
      "Read constituents (2387) and vectors (2387):\n",
      "Constituent labels and order in both files match\n",
      "Read compounds (5988) and vectors (5988):\n",
      "Compound labels and order in both files match\n",
      "Vector matrix shape:  (942,) 300\n",
      "Read constituents (942) and vectors (942):\n",
      "Constituent labels and order in both files match\n",
      "Read compounds (578) and vectors (578):\n",
      "Compound labels and order in both files match\n",
      "Total common Constituent vectors ::  825\n",
      "Total common Compound vectors :: 553\n",
      "Total vectors :: 1378\n",
      "*****************************\n"
     ]
    }
   ],
   "source": [
    "# Give filenames and filepaths for TEXT or VISION\n",
    "\n",
    "# Variables (Also used to give filenames dynamically)\n",
    "processes = ['textvision']\n",
    "normalizations = [False, True]\n",
    "\n",
    "for process in processes:\n",
    "    print(\"PROCESS :: \", process)\n",
    "\n",
    "    if process == \"textvision\":\n",
    "        print(\"Processing text vision data\")\n",
    "\n",
    "        t_constituent_list = \"./data/__text_vectors/text_constituents.list.txt\"\n",
    "        t_constituent_vectors = \"./data/__text_vectors/text_constituents.vectors.txt\"\n",
    "        t_compound_list = \"./data/__text_vectors/text_noun-compounds.list.txt\"\n",
    "        t_compound_vectors = \"./data//__text_vectors/text_noun-compounds.vectors.txt\"\n",
    "        \n",
    "        t_const_list, t_const_vectors = get_constituent_data(t_constituent_list,t_constituent_vectors)\n",
    "        t_comp_list, t_comp_vectors = get_compound_data(t_compound_list, t_compound_vectors)\n",
    "\n",
    "        v_constituent_list = \"./data/__vision_vectors/vision_constituents.list.txt\"\n",
    "        v_constituent_vectors = \"./data/__vision_vectors/vision_constituents.vectors.txt\"\n",
    "        v_compound_list = \"./data/__vision_vectors/vision_noun-compounds.list.txt\"\n",
    "        v_compound_vectors = \"./data/__vision_vectors/vision_noun-compounds.vectors.txt\"\n",
    "        \n",
    "        v_const_list, v_const_vectors = get_constituent_data(v_constituent_list,v_constituent_vectors)\n",
    "        v_comp_list, v_comp_vectors = get_compound_data(v_compound_list, v_compound_vectors)\n",
    "\n",
    "        const_vectors, ct_labels, ct_tvecs, ct_vvecs = concatenate_multimodal_data(t_const_vectors, v_const_vectors, 'constituents', 'word')\n",
    "        comp_vectors, cp_labels, cp_tvecs, cp_vvecs = concatenate_multimodal_data(t_comp_vectors, v_comp_vectors, 'compound', 'word')\n",
    "        \n",
    "    else:\n",
    "\n",
    "        print(\"NOTHING HERE\")\n",
    "    \n",
    "    print(\"Total common Constituent vectors :: \",  len(const_vectors))\n",
    "    print(\"Total common Compound vectors ::\", len(comp_vectors))\n",
    "    print(\"Total vectors ::\", len(const_vectors) + len(comp_vectors))\n",
    "    \n",
    "    all_vecs = pd.concat([const_vectors, comp_vectors])\n",
    "    labels = ct_labels + cp_labels\n",
    "    tvecs = ct_tvecs + cp_tvecs\n",
    "    vvecs = ct_vvecs + cp_vvecs\n",
    "    \n",
    "    print(\"*****************************\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and plot \n",
    "# Text and vision -- unnormalized data\n",
    "plt.clf()\n",
    "\n",
    "tvecs_red = PCA(n_components=2).fit_transform(tvecs)\n",
    "ttvecs_red = PCA(n_components=2).fit_transform(normalize(tvecs, norm=\"l2\", axis=1))\n",
    "vvecs_red = PCA(n_components=2).fit_transform(vvecs)\n",
    "\n",
    "plt.scatter(tvecs_red[:, 0], tvecs_red[:, 1], color = 'blue', s=1)\n",
    "plt.scatter(ttvecs_red[:, 0], ttvecs_red[:, 1], color = 'yellow', s=1)\n",
    "plt.title(\"Unnormalized vs Normalized Text Space\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and plot \n",
    "# Text and vision -- unnormalized data\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "tvecs_red = PCA(n_components=2).fit_transform(tvecs)\n",
    "vvecs_red = PCA(n_components=2).fit_transform(vvecs)\n",
    "vvvecs_red = PCA(n_components=2).fit_transform(normalize(vvecs, norm=\"l2\", axis=1))\n",
    "\n",
    "plt.scatter(vvecs_red[:, 0], vvecs_red[:, 1], color = 'red', s=1)\n",
    "plt.scatter(vvvecs_red[:, 0], vvvecs_red[:, 1], color = 'green', s=40)\n",
    "plt.title(\"Unnormalized vs Normalized Vision Space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and plot \n",
    "# Text and vision -- unnormalized data\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "tvecs_red = PCA(n_components=2).fit_transform(tvecs)\n",
    "vvecs_red = PCA(n_components=2).fit_transform(vvecs)\n",
    "\n",
    "plt.scatter(tvecs_red[:, 0], tvecs_red[:, 1], color = 'blue', s=1)\n",
    "plt.scatter(vvecs_red[:, 0], vvecs_red[:, 1], color = 'red', s=1)\n",
    "plt.title(\"Unnormalized TEXT & VISION Space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inter space (Non-sensical)\n",
    "tvcos = [dot(tvecs_red[i], vvecs_red[i])/(norm(tvecs_red[i])*norm(vvecs_red[i])) for i in range(len(tvecs_red))]\n",
    "\n",
    "print(\"This number doesn't mean anything -- the dimensions accross spaces have NO correspondence\")\n",
    "print(\"BUT, its a good indicator for the 'actual' similarities later on\")\n",
    "print(np.mean(tvcos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and plot\n",
    "# Text and vision -- NORMALIZED  \n",
    "\n",
    "plt.clf()\n",
    "\n",
    "tvecs_red = PCA(n_components=2).fit_transform(normalize(tvecs, norm=\"l2\", axis=1))\n",
    "vvecs_red = PCA(n_components=2).fit_transform(normalize(vvecs, norm=\"l2\", axis=1))\n",
    "\n",
    "plt.scatter(tvecs_red[:, 0], tvecs_red[:, 1], color = 'blue', s=1)\n",
    "plt.scatter(vvecs_red[:, 0], vvecs_red[:, 1], color = 'red', s=1)\n",
    "plt.title(\"Normalized TEXT and VISION Space\")\n",
    "plt.show()\n",
    "print(\"These are the 2 spaces that we are acutally interested in and, we'll use these later on for CCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inter space (Non-sensical)\n",
    "tvcos = [dot(tvecs_red[i], vvecs_red[i])/(norm(tvecs_red[i])*norm(vvecs_red[i])) for i in range(len(tvecs_red))]\n",
    "print(np.mean(tvcos))\n",
    "print(\"Improved cosine similarity BUT still non-sensical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCA and plot \n",
    "# Text and vision -- unnormalized data\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "tvecs_cca, vvecs_cca = CCA(n_components=2).fit_transform(tvecs, vvecs)\n",
    "\n",
    "plt.scatter(tvecs_cca[:, 0], tvecs_cca[:, 1], color = 'blue', s=1)\n",
    "plt.scatter(vvecs_cca[:, 0], vvecs_cca[:, 1], color = 'red', s=1)\n",
    "plt.title(\"CCA Transformed Unnormalized TEXT & VISION Space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCA and plot \n",
    "# Text and vision -- NORMALIZED\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "tvecs_cca, vvecs_cca = CCA(n_components=2).fit_transform(normalize(tvecs, norm=\"l2\", axis=1), normalize(vvecs, norm=\"l2\", axis=1))\n",
    "\n",
    "plt.scatter(tvecs_cca[:, 0], tvecs_cca[:, 1], color = 'blue', s=1)\n",
    "plt.scatter(vvecs_cca[:, 0], vvecs_cca[:, 1], color = 'red', s=1)\n",
    "plt.title(\"CCA Transformed Normalized TEXT & VISION Space\")\n",
    "plt.show()\n",
    "\n",
    "print(\"GOOD that Unnormalized vs normalized spaces (the above 2 plots) are not VERY different\")\n",
    "print(\"because, covariances should be the same in the two spaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarities(dim=None, normalization=None, max_iter=None, space1=None, space2=None):\n",
    "    if not normalization: \n",
    "        space1_cca, space2_cca = CCA(n_components=dim, max_iter=max_iter).fit_transform(space1, space2)\n",
    "    else:\n",
    "        space1_cca, space2_cca = CCA(n_components=dim, max_iter=max_iter).fit_transform(normalize(space1, norm=\"l2\", axis=1), normalize(space2, norm=\"l2\", axis=1))\n",
    "    \n",
    "    s1s2cos = [dot(space1_cca[i], space2_cca[i])/(norm(space1_cca[i])*norm(space2_cca[i])) for i in range(len(space1_cca))]\n",
    "#     print(np.mean(s1s2cos), np.median(s1s2cos), np.std(s1s2cos), dim, normalization, max_iter)\n",
    "    \n",
    "    return (np.median(s1s2cos), np.mean(s1s2cos), np.std(s1s2cos), dim, normalization, max_iter)\n",
    "\n",
    "\n",
    "print(\"This particular block will take some time to compute, IF dims is a long list\")\n",
    "dims = [50,300]\n",
    "max_iters = [500,1000,5000]\n",
    "normalizations = [True, False]\n",
    "\n",
    "sims = sorted([print_similarities(dim=dim, normalization=normalization, max_iter=max_iter, space1=tvecs, space2=vvecs) for dim in dims for max_iter in max_iters for normalization in normalizations], reverse=True)\n",
    "\n",
    "print(sims)\n",
    "\n",
    "print(\"These are the actual similarities that have been computed for 2 spaces\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average cosine similarities of the ORIGINAL UNNORMALIZED TEXT and VISION spaces\n",
      "0.097473024470814 0.666321643872867\n"
     ]
    }
   ],
   "source": [
    "# Intra space - cosine similarities \n",
    "tcos_sim = cosine_similarity(tvecs)\n",
    "vcos_sim = cosine_similarity(vvecs)\n",
    "\n",
    "print(\"The average cosine similarities of the ORIGINAL UNNORMALIZED TEXT and VISION spaces\")\n",
    "print(np.mean(tcos_sim), np.mean(vcos_sim))\n",
    "# print(np.median(tcos_sim), np.median(vcos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average cosine similarities of the ORIGINAL NORMALIZED TEXT and VISION spaces\n",
      "0.09747302447081402 0.6663216438728671\n",
      " \n",
      "ITs good that the similarity scores are almost indenticial for the unnormalized and normalized spaces\n"
     ]
    }
   ],
   "source": [
    "# Intra space - cosine similarities \n",
    "tcos_sim = cosine_similarity(normalize(tvecs, norm=\"l2\", axis=1))\n",
    "vcos_sim = cosine_similarity(normalize(vvecs, norm=\"l2\", axis=1))\n",
    "\n",
    "print(\"The average cosine similarities of the ORIGINAL NORMALIZED TEXT and VISION spaces\")\n",
    "print(np.mean(tcos_sim), np.mean(vcos_sim))\n",
    "# print(np.median(tcos_sim), np.median(vcos_sim))\n",
    "\n",
    "print(\" \")\n",
    "print(\"ITs good that the similarity scores are almost indenticial for the unnormalized and normalized spaces\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarities(dim=None, normalization=None, max_iter=None, space1=None, space2=None):\n",
    "    print(\"computing CCA\")\n",
    "    if not normalization: \n",
    "        space1_cca, space2_cca = CCA(n_components=dim, max_iter=max_iter).fit_transform(space1, space2)\n",
    "    \n",
    "    else:\n",
    "        space1_cca, space2_cca = CCA(n_components=dim, max_iter=max_iter).fit_transform(normalize(space1, norm=\"l2\", axis=1), normalize(space2, norm=\"l2\", axis=1))\n",
    "    \n",
    "    print(\"computing intra-space sims\")\n",
    "    tcos_sim = cosine_similarity(space1_cca)\n",
    "    vcos_sim = cosine_similarity(space2_cca)\n",
    "    \n",
    "    print(\"computing inter-space sims\")\n",
    "    s1s2cos = [dot(space1_cca[i], space2_cca[i])/(norm(space1_cca[i])*norm(space2_cca[i])) for i in range(len(space1_cca))]\n",
    "     \n",
    "#     print(np.mean(tcos_sim), np.mean(vcos_sim))\n",
    "#     print(np.median(tcos_sim), np.median(vcos_sim))\n",
    "\n",
    "    return (np.median(s1s2cos), np.mean(s1s2cos), np.std(s1s2cos), dim, normalization, max_iter, np.mean(tcos_sim), np.mean(vcos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_similarities(dim=50, normalization=True, max_iter=500, space1=tvecs, space2=vvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing CCA\n",
      "computing intra-space sims\n",
      "computing inter-space sims\n",
      "Start\n",
      "computing CCA\n",
      "computing intra-space sims\n",
      "computing inter-space sims\n",
      "computing CCA\n",
      "computing intra-space sims\n",
      "computing inter-space sims\n",
      "END\n",
      "(0.8390147965552441, 0.8282288362086534, 0.07366431368297294, 50, True, 1000, 0.00014857197603201022, 0.0003600175998597654)\n",
      "(0.8263470336601949, 0.8020594462535744, 0.11401640663970537, 50, False, 1000, 0.0003918802856918176, 0.00043784019872703726)\n"
     ]
    }
   ],
   "source": [
    "# Inter-space similarities, post CCA\n",
    "dims = [50]\n",
    "max_iters = [1000]\n",
    "normalizations = [True, False]\n",
    "\n",
    "print(\"Start\")\n",
    "sims = sorted([print_similarities(dim=dim, normalization=normalization, max_iter=max_iter, space1=tvecs, space2=vvecs) for dim in dims for max_iter in max_iters for normalization in normalizations], reverse=True)\n",
    "print(\"END\")\n",
    "\n",
    "for eachSim in sims:\n",
    "    print(eachSim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
